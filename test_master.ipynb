{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_master.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLDYX7f3R8PM"
      },
      "source": [
        "Импорт нужных модулей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSu0tV9mMFDK"
      },
      "source": [
        "from google.colab import drive\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.metrics import *\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.metrics import *\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vkpG3tofUp5",
        "outputId": "44cf5a73-3cdb-4563-a197-e0803b656611"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXjE6iUiV0ZY"
      },
      "source": [
        "Анализ датасета.\n",
        "Для этого, в основном, будут использоваться инструменты pandas dataframe. А также библиотек sklearn и seaborn.\n",
        "\n",
        "===========================================================\n",
        "\n",
        "Dataset Analysis\n",
        "Main tool is pandas dataframe, sklearn and seaborn as instrument for nice heatmap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNr_Wif9V2kE",
        "outputId": "66b227a1-34c6-4dfb-d0e0-785235496333"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "#Загрузка датасета по частям с дальнейшей конкатенацией. \n",
        "df1=pd.read_csv(\"/content/drive/MyDrive/CICIDS2017/data_analysis/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")#,nrows = 50000\n",
        "df2=pd.read_csv(\"/content/drive/MyDrive/CICIDS2017/data_analysis/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\n",
        "df3=pd.read_csv(\"/content/drive/MyDrive/CICIDS2017/data_analysis/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n",
        "df4=pd.read_csv(\"/content/drive/MyDrive/CICIDS2017/data_analysis/Monday-WorkingHours.pcap_ISCX.csv\")\n",
        "df5=pd.read_csv(\"/content/drive/MyDrive/CICIDS2017/data_analysis/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\n",
        "df6=pd.read_csv(\"/content/drive/MyDrive/CICIDS2017/data_analysis/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
        "df7=pd.read_csv(\"/content/drive/MyDrive/CICIDS2017/data_analysis/Tuesday-WorkingHours.pcap_ISCX.csv\")\n",
        "df8=pd.read_csv(\"/content/drive/MyDrive/CICIDS2017/data_analysis/Wednesday-workingHours.pcap_ISCX.csv\")\n",
        "\n",
        "#Вывод уникальных признаков типов аттак или benign для каждого датасета. Ожидается DDoS, DoS Types и т.д.\n",
        "print (df1[' Label'].unique())\n",
        "print (df2[' Label'].unique())\n",
        "print (df3[' Label'].unique())\n",
        "print (df4[' Label'].unique())\n",
        "print (df5[' Label'].unique())\n",
        "print (df6[' Label'].unique())\n",
        "print (df7[' Label'].unique())\n",
        "print (df8[' Label'].unique(), \"\\n\")\n",
        "\n",
        "#Конкатенация и общая информация о наборе данных\n",
        "df = pd.concat([df1,df2])\n",
        "del df1,df2\n",
        "df = pd.concat([df,df3])\n",
        "del df3\n",
        "df= pd.concat([df,df4])\n",
        "del df4\n",
        "df = pd.concat([df,df5])\n",
        "del df5\n",
        "df = pd.concat([df,df6])\n",
        "del df6\n",
        "df = pd.concat([df,df7])\n",
        "del df7\n",
        "df = pd.concat([df,df8])\n",
        "del df8\n",
        "data = df.copy() #Датафрейм копируем в дату для дальнейших операций\n",
        "data.info()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BENIGN' 'DDoS']\n",
            "['BENIGN' 'PortScan']\n",
            "['BENIGN' 'Bot']\n",
            "['BENIGN']\n",
            "['BENIGN' 'Infiltration']\n",
            "['BENIGN' 'Web Attack � Brute Force' 'Web Attack � XSS'\n",
            " 'Web Attack � Sql Injection']\n",
            "['BENIGN' 'FTP-Patator' 'SSH-Patator']\n",
            "['BENIGN' 'DoS slowloris' 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye'\n",
            " 'Heartbleed'] \n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2830743 entries, 0 to 692702\n",
            "Data columns (total 79 columns):\n",
            " #   Column                        Dtype  \n",
            "---  ------                        -----  \n",
            " 0    Destination Port             int64  \n",
            " 1    Flow Duration                int64  \n",
            " 2    Total Fwd Packets            int64  \n",
            " 3    Total Backward Packets       int64  \n",
            " 4   Total Length of Fwd Packets   int64  \n",
            " 5    Total Length of Bwd Packets  int64  \n",
            " 6    Fwd Packet Length Max        int64  \n",
            " 7    Fwd Packet Length Min        int64  \n",
            " 8    Fwd Packet Length Mean       float64\n",
            " 9    Fwd Packet Length Std        float64\n",
            " 10  Bwd Packet Length Max         int64  \n",
            " 11   Bwd Packet Length Min        int64  \n",
            " 12   Bwd Packet Length Mean       float64\n",
            " 13   Bwd Packet Length Std        float64\n",
            " 14  Flow Bytes/s                  float64\n",
            " 15   Flow Packets/s               float64\n",
            " 16   Flow IAT Mean                float64\n",
            " 17   Flow IAT Std                 float64\n",
            " 18   Flow IAT Max                 int64  \n",
            " 19   Flow IAT Min                 int64  \n",
            " 20  Fwd IAT Total                 int64  \n",
            " 21   Fwd IAT Mean                 float64\n",
            " 22   Fwd IAT Std                  float64\n",
            " 23   Fwd IAT Max                  int64  \n",
            " 24   Fwd IAT Min                  int64  \n",
            " 25  Bwd IAT Total                 int64  \n",
            " 26   Bwd IAT Mean                 float64\n",
            " 27   Bwd IAT Std                  float64\n",
            " 28   Bwd IAT Max                  int64  \n",
            " 29   Bwd IAT Min                  int64  \n",
            " 30  Fwd PSH Flags                 int64  \n",
            " 31   Bwd PSH Flags                int64  \n",
            " 32   Fwd URG Flags                int64  \n",
            " 33   Bwd URG Flags                int64  \n",
            " 34   Fwd Header Length            int64  \n",
            " 35   Bwd Header Length            int64  \n",
            " 36  Fwd Packets/s                 float64\n",
            " 37   Bwd Packets/s                float64\n",
            " 38   Min Packet Length            int64  \n",
            " 39   Max Packet Length            int64  \n",
            " 40   Packet Length Mean           float64\n",
            " 41   Packet Length Std            float64\n",
            " 42   Packet Length Variance       float64\n",
            " 43  FIN Flag Count                int64  \n",
            " 44   SYN Flag Count               int64  \n",
            " 45   RST Flag Count               int64  \n",
            " 46   PSH Flag Count               int64  \n",
            " 47   ACK Flag Count               int64  \n",
            " 48   URG Flag Count               int64  \n",
            " 49   CWE Flag Count               int64  \n",
            " 50   ECE Flag Count               int64  \n",
            " 51   Down/Up Ratio                int64  \n",
            " 52   Average Packet Size          float64\n",
            " 53   Avg Fwd Segment Size         float64\n",
            " 54   Avg Bwd Segment Size         float64\n",
            " 55   Fwd Header Length.1          int64  \n",
            " 56  Fwd Avg Bytes/Bulk            int64  \n",
            " 57   Fwd Avg Packets/Bulk         int64  \n",
            " 58   Fwd Avg Bulk Rate            int64  \n",
            " 59   Bwd Avg Bytes/Bulk           int64  \n",
            " 60   Bwd Avg Packets/Bulk         int64  \n",
            " 61  Bwd Avg Bulk Rate             int64  \n",
            " 62  Subflow Fwd Packets           int64  \n",
            " 63   Subflow Fwd Bytes            int64  \n",
            " 64   Subflow Bwd Packets          int64  \n",
            " 65   Subflow Bwd Bytes            int64  \n",
            " 66  Init_Win_bytes_forward        int64  \n",
            " 67   Init_Win_bytes_backward      int64  \n",
            " 68   act_data_pkt_fwd             int64  \n",
            " 69   min_seg_size_forward         int64  \n",
            " 70  Active Mean                   float64\n",
            " 71   Active Std                   float64\n",
            " 72   Active Max                   int64  \n",
            " 73   Active Min                   int64  \n",
            " 74  Idle Mean                     float64\n",
            " 75   Idle Std                     float64\n",
            " 76   Idle Max                     int64  \n",
            " 77   Idle Min                     int64  \n",
            " 78   Label                        object \n",
            "dtypes: float64(24), int64(54), object(1)\n",
            "memory usage: 1.7+ GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0soqw_7WZp78"
      },
      "source": [
        "<h4>Необязательный шаг.</h4>\n",
        "\n",
        "Тепловая карта для сырого датасета. Благодаря ней можно понять основные недостатки сырого датасета и с ними работать в дальнейшем препроцессинге."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_borUBAbxVz"
      },
      "source": [
        "#fig= plt.figure(figsize=(70,70))\n",
        "#sns.heatmap(data.corr(), annot=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqqcYR9qjqt1"
      },
      "source": [
        "Необходимо отбросить пустые значения из изначального датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzUrltRsypXi"
      },
      "source": [
        "#Подтягиваем из датасета колонки с NaN и создаем из них массив deleteCol. Получившийся массив \"вычитаем\" из дата.\n",
        "deleteCol = []\n",
        "for column in data.columns:\n",
        "    if data[column].isnull().values.any():\n",
        "        deleteCol.append(column)\n",
        "for column in deleteCol:\n",
        "    data.drop([column],axis=1,inplace=True)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX8L5M96rKQz"
      },
      "source": [
        "Аналогично поступаем с элементами numpy.object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elrIJl3Umfpq"
      },
      "source": [
        "deleteCol = []\n",
        "for column in data.columns:\n",
        "    if column == ' Label':\n",
        "        continue\n",
        "    elif data[column].dtype==np.object:\n",
        "        deleteCol.append(column)\n",
        "for column in deleteCol:\n",
        "    data.drop(column,axis=1,inplace=True)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLjm452FsWCU"
      },
      "source": [
        "Выявим уникальные значения метрики Flow Duration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDJpmleJmhfo",
        "outputId": "fedf061a-dfd7-4cc7-e5af-0b19dcbefedf"
      },
      "source": [
        "data[' Flow Duration'].unique()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([       3,      109,       52, ..., 11509095, 11512230,  1048635])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEbI2mUPsibk"
      },
      "source": [
        "Привидем каждый столбец к единому числовому типу на основе максимального значения в выбранном столбце."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoi6WgAki6rl",
        "outputId": "60198a07-4870-455a-fa75-c3cc9b19eaaf"
      },
      "source": [
        "#int8   от отрицательного -128 до положительного 127\n",
        "#int16  от отрицательного -32768 до положительных 32767\n",
        "#int32  от отрицательного 2 147 483 648 до положительного 2 147 483 647\n",
        "\n",
        "for column in data.columns:\n",
        "    if data[column].dtype == np.int64:\n",
        "        maxVal = data[column].max()\n",
        "        if maxVal < 120:\n",
        "            data[column] = data[column].astype(np.int8)\n",
        "        elif maxVal < 32767:\n",
        "            data[column] = data[column].astype(np.int16)\n",
        "        else:\n",
        "            data[column] = data[column].astype(np.int32)\n",
        "            \n",
        "    if data[column].dtype == np.float64:\n",
        "        maxVal = data[column].max()\n",
        "        minVal = data[data[column]>0][column]\n",
        "        if maxVal < 120 and minVal>0.01 :\n",
        "            data[column] = data[column].astype(np.float16)\n",
        "        else:\n",
        "            data[column] = data[column].astype(np.float32)\n",
        "#Проверим типы данных\n",
        "data.info()\n",
        "#Проверим количество маркированных пакетов\n",
        "print(data[\" Label\"].value_counts(), \"\\n\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2830743 entries, 0 to 692702\n",
            "Data columns (total 78 columns):\n",
            " #   Column                        Dtype  \n",
            "---  ------                        -----  \n",
            " 0    Destination Port             int32  \n",
            " 1    Flow Duration                int32  \n",
            " 2    Total Fwd Packets            int32  \n",
            " 3    Total Backward Packets       int32  \n",
            " 4   Total Length of Fwd Packets   int32  \n",
            " 5    Total Length of Bwd Packets  int32  \n",
            " 6    Fwd Packet Length Max        int16  \n",
            " 7    Fwd Packet Length Min        int16  \n",
            " 8    Fwd Packet Length Mean       float32\n",
            " 9    Fwd Packet Length Std        float32\n",
            " 10  Bwd Packet Length Max         int16  \n",
            " 11   Bwd Packet Length Min        int16  \n",
            " 12   Bwd Packet Length Mean       float32\n",
            " 13   Bwd Packet Length Std        float32\n",
            " 14   Flow Packets/s               float32\n",
            " 15   Flow IAT Mean                float32\n",
            " 16   Flow IAT Std                 float32\n",
            " 17   Flow IAT Max                 int32  \n",
            " 18   Flow IAT Min                 int32  \n",
            " 19  Fwd IAT Total                 int32  \n",
            " 20   Fwd IAT Mean                 float32\n",
            " 21   Fwd IAT Std                  float32\n",
            " 22   Fwd IAT Max                  int32  \n",
            " 23   Fwd IAT Min                  int32  \n",
            " 24  Bwd IAT Total                 int32  \n",
            " 25   Bwd IAT Mean                 float32\n",
            " 26   Bwd IAT Std                  float32\n",
            " 27   Bwd IAT Max                  int32  \n",
            " 28   Bwd IAT Min                  int32  \n",
            " 29  Fwd PSH Flags                 int8   \n",
            " 30   Bwd PSH Flags                int8   \n",
            " 31   Fwd URG Flags                int8   \n",
            " 32   Bwd URG Flags                int8   \n",
            " 33   Fwd Header Length            int32  \n",
            " 34   Bwd Header Length            int32  \n",
            " 35  Fwd Packets/s                 float32\n",
            " 36   Bwd Packets/s                float32\n",
            " 37   Min Packet Length            int16  \n",
            " 38   Max Packet Length            int16  \n",
            " 39   Packet Length Mean           float32\n",
            " 40   Packet Length Std            float32\n",
            " 41   Packet Length Variance       float32\n",
            " 42  FIN Flag Count                int8   \n",
            " 43   SYN Flag Count               int8   \n",
            " 44   RST Flag Count               int8   \n",
            " 45   PSH Flag Count               int8   \n",
            " 46   ACK Flag Count               int8   \n",
            " 47   URG Flag Count               int8   \n",
            " 48   CWE Flag Count               int8   \n",
            " 49   ECE Flag Count               int8   \n",
            " 50   Down/Up Ratio                int16  \n",
            " 51   Average Packet Size          float32\n",
            " 52   Avg Fwd Segment Size         float32\n",
            " 53   Avg Bwd Segment Size         float32\n",
            " 54   Fwd Header Length.1          int32  \n",
            " 55  Fwd Avg Bytes/Bulk            int8   \n",
            " 56   Fwd Avg Packets/Bulk         int8   \n",
            " 57   Fwd Avg Bulk Rate            int8   \n",
            " 58   Bwd Avg Bytes/Bulk           int8   \n",
            " 59   Bwd Avg Packets/Bulk         int8   \n",
            " 60  Bwd Avg Bulk Rate             int8   \n",
            " 61  Subflow Fwd Packets           int32  \n",
            " 62   Subflow Fwd Bytes            int32  \n",
            " 63   Subflow Bwd Packets          int32  \n",
            " 64   Subflow Bwd Bytes            int32  \n",
            " 65  Init_Win_bytes_forward        int32  \n",
            " 66   Init_Win_bytes_backward      int32  \n",
            " 67   act_data_pkt_fwd             int32  \n",
            " 68   min_seg_size_forward         int16  \n",
            " 69  Active Mean                   float32\n",
            " 70   Active Std                   float32\n",
            " 71   Active Max                   int32  \n",
            " 72   Active Min                   int32  \n",
            " 73  Idle Mean                     float32\n",
            " 74   Idle Std                     float32\n",
            " 75   Idle Max                     int32  \n",
            " 76   Idle Min                     int32  \n",
            " 77   Label                        object \n",
            "dtypes: float32(23), int16(8), int32(28), int8(18), object(1)\n",
            "memory usage: 685.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q_jJrdKwSZn"
      },
      "source": [
        "Подчистим датасет от пустых и бесконечных значений."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgfotGSsvI7t"
      },
      "source": [
        "# Проверим количество потерянных значений в датасете\n",
        "print(f\"Missing values: {data.isnull().sum().sum()}\", \"\\n\")\n",
        "#Проверим датасет на бесконечные значения. Заменим их на NaN, так как их потом просто удалить.\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "print(f\"Missing values: {data.isnull().sum().sum()}\", \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox7UALTQwaC9"
      },
      "source": [
        "Ключевая проверка датасета на кол-во маркированных пакетов для последующего семплирования."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu-dR2u0wNbk"
      },
      "source": [
        "print(data[' Label'].value_counts(), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4WTfQkOwhMO"
      },
      "source": [
        "<h3>Семплирование</h3>\n",
        "Процедура предназначена для проведения качественного обучения НС в заданных соотношениях. То есть, мы исключим ситуацию, где из 100 элементов 95% - одного типа. \n",
        "\n",
        "По дефолту оставил 0.5, т.е. из всех оставшихся строк, маркированных как, допустим, \"Heartbleed\", будут выбраны 50%. В дальнейшем удобно корректтировать их семплирование, благодаря громоздкому реплицированному коду."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh5gKZx8wFfV"
      },
      "source": [
        "\n",
        "benign = data[data[' Label'] == 'BENIGN'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'BENIGN']\n",
        "data = pd.concat([attack, benign])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "ddos = data[data[' Label'] == 'DDoS'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'DDoS']\n",
        "data = pd.concat([attack, ddos])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "PortScan = data[data[' Label'] == 'PortScan'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'PortScan']\n",
        "data = pd.concat([attack, PortScan])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "Bot = data[data[' Label'] == 'Bot'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'Bot']\n",
        "data = pd.concat([attack, Bot])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "Infiltration = data[data[' Label'] == 'Infiltration'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'Infiltration']\n",
        "data = pd.concat([attack, Infiltration])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "BF = data[data[' Label'] == 'Web Attack � Brute Force'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'Web Attack � Brute Force']\n",
        "data = pd.concat([attack, BF])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "XSS = data[data[' Label'] == 'Web Attack � XSS'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'Web Attack � XSS']\n",
        "data = pd.concat([attack, XSS])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "SQLI = data[data[' Label'] == 'Web Attack � Sql Injection'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'Web Attack � Sql Injection']\n",
        "data = pd.concat([attack, SQLI])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "FTPP = data[data[' Label'] == 'FTP-Patator'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'FTP-Patator']\n",
        "data = pd.concat([attack, FTPP])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "SSHP = data[data[' Label'] == 'SSH-Patator'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'SSH-Patator']\n",
        "data = pd.concat([attack, SSHP])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "DOSs = data[data[' Label'] == 'DoS slowloris'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'DoS slowloris']\n",
        "data = pd.concat([attack, DOSs])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "DOShttp = data[data[' Label'] == 'DoS Slowhttptest'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'DoS Slowhttptest']\n",
        "data = pd.concat([attack, DOShttp])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "Hulk = data[data[' Label'] == 'DoS Hulk'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'DoS Hulk']\n",
        "data = pd.concat([attack, Hulk])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "GY = data[data[' Label'] == 'DoS GoldenEye'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'DoS GoldenEye']\n",
        "data = pd.concat([attack, GY])\n",
        "#print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "HB = data[data[' Label'] == 'Heartbleed'].sample(frac=0.5).reset_index(drop=True)\n",
        "attack = data[data[' Label'] != 'Heartbleed']\n",
        "data = pd.concat([attack, HB])\n",
        "print(data[' Label'].value_counts(), \"\\n\")\n",
        "\n",
        "y = data[' Label']\n",
        "X = data.drop([' Label'],axis=1)\n",
        "\n",
        "\n",
        "fig= plt.figure(figsize=(70,70))\n",
        "sns.heatmap(X.corr(), annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "uggMFGgIZryX",
        "outputId": "3373b507-2a8c-43cd-a91f-a0673676362e"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "bestfeatures = SelectKBest(score_func=f_classif, k=10)\n",
        "fit = bestfeatures.fit(X,y)\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(X.columns)\n",
        "#concat two dataframes for better visualization \n",
        "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
        "print(featureScores.nlargest(30,'Score'))  #print 10 best features\n",
        "\n",
        "feature = pd.DataFrame()\n",
        "n = len(featureScores['Specs'])\n",
        "for i in featureScores.nlargest(n//2,'Score')['Specs']:\n",
        "        feature[i] = data[i]\n",
        "feature[' Label'] = data[' Label']\n",
        "\n",
        "\n",
        "\n",
        "fig= plt.figure(figsize=(40,40))\n",
        "sns.heatmap(feature.corr(), annot=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-27d2345615f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf_classif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbestfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_classif\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbestfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdfscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdfcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \"\"\"\n\u001b[1;32m    397\u001b[0m         X, y = self._validate_data(\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         )\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m     )\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    114\u001b[0m             raise ValueError(\n\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 )\n\u001b[1;32m    118\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GNdLr-wSD7k"
      },
      "source": [
        "Объявление переменных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXeYRrdAKXLr"
      },
      "source": [
        "path = '/content/drive/MyDrive/CICIDS2017/data_nn'\n",
        "data = []\n",
        "target = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsfczGW6SIp7"
      },
      "source": [
        "Чтение датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhXcfO4oaRi4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46eb0837-1239-4c6d-fe05-d80b9c158d86"
      },
      "source": [
        "for name in os.listdir(path):\n",
        "    file = pd.read_csv(os.path.join(path, name))\n",
        "    print ('Types of marks in', name, \":\", file[' Label'].unique())\n",
        "    for n, i in file.iterrows():\n",
        "        a = []\n",
        "        for j in i[:-1]:\n",
        "          a.append(j)\n",
        "        data.append(a) \n",
        "        target.append(0 if i[-1] == 'BENIGN' else 1) \n",
        "del file\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Types of marks in Wednesday-workingHours.pcap_ISCX.csv : ['BENIGN' 'DoS slowloris' 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye'\n",
            " 'Heartbleed']\n",
            "Types of marks in Tuesday-WorkingHours.pcap_ISCX.csv : ['BENIGN' 'FTP-Patator' 'SSH-Patator']\n",
            "Types of marks in Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv : ['BENIGN' 'Web Attack � Brute Force' 'Web Attack � XSS'\n",
            " 'Web Attack � Sql Injection']\n",
            "Types of marks in Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv : ['BENIGN' 'Infiltration']\n",
            "Types of marks in Monday-WorkingHours.pcap_ISCX.csv : ['BENIGN']\n",
            "Types of marks in Friday-WorkingHours-Morning.pcap_ISCX.csv : ['BENIGN' 'Bot']\n",
            "Types of marks in Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv : ['BENIGN' 'PortScan']\n",
            "Types of marks in Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv : ['BENIGN' 'DDoS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A34qan4rTza_"
      },
      "source": [
        "Обработка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq7rb8uKTEH2",
        "outputId": "a8fc73cb-65b3-4a72-be01-ad1f535d2612"
      },
      "source": [
        "data = np.array(data)\n",
        "target = np.array(target)\n",
        "print(data.shape, target.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2830743, 78) (2830743,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiooPSVpAiId"
      },
      "source": [
        "Подготовка данных для нормализации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5grlnWHM8yq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb046551-5be2-4cb8-d79f-a539ab0a97da"
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "scaler.fit(np.nan_to_num(data).astype(float))\n",
        "data = scaler.transform(np.nan_to_num(data).astype(float))\n",
        "data = data.reshape(data.shape[0], 78, 1)\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvH6ZgW-xHaQ"
      },
      "source": [
        "datax = data.reshape(data.shape[0], 78)\n",
        "ax = sns.heatmap(datax) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUV3cphYAqhU"
      },
      "source": [
        "Сохранение нормализатора"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWT07_vYGR1r"
      },
      "source": [
        "pickle.dump(scaler, open('/content/drive/MyDrive/CICIDS2017/devtest/scaler.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhCFTFSwSQiT"
      },
      "source": [
        "Моделирование нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00fuAt0ONpiD"
      },
      "source": [
        "lstm = Sequential()\n",
        "lstm.add(Input((78, 1)))\n",
        "lstm.add(LSTM(32, recurrent_activation='sigmoid', return_sequences=True, recurrent_dropout=0))\n",
        "for i in range(3):\n",
        "  lstm.add(LSTM(32, recurrent_activation='sigmoid', return_sequences=True, recurrent_dropout=0))\n",
        "lstm.add(Flatten())\n",
        "lstm.add(Dense(2,activation = 'softmax'))\n",
        "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEtpubU7T5L3"
      },
      "source": [
        "Тренировка нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz_Z40v3Tpws"
      },
      "source": [
        "lstm.fit(data, to_categorical(target), epochs=2, batch_size=256, verbose=1)\n",
        "\n",
        "lstm.save('/content/drive/MyDrive/CICIDS2017/devtest/lstm.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHCQU3U0IpOn"
      },
      "source": [
        "Получение данных от нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfcQ8hJ8EAAQ"
      },
      "source": [
        "y_pred1 = lstm.predict(data, batch_size=1024)\n",
        "y_pred = np.argmax(y_pred1, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-_VYSkLItOW"
      },
      "source": [
        "Вычисление метрик и количества пакетов с атакой и без"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1x6fhcqEGwn"
      },
      "source": [
        "print('recall:', recall_score(target, y_pred, average='micro'))\n",
        "print('precision:', precision_score(target, y_pred, average='micro'))\n",
        "print('accuracy:', accuracy_score(target, y_pred))\n",
        "print('f1 score:', f1_score(target, y_pred, average='micro'))\n",
        "print('ROC AUC:', roc_auc_score(target, y_pred))\n",
        "print('Confussion Matrix', confusion_matrix(target, y_pred))\n",
        "unique, counts = np.unique(y_pred, return_counts=True)\n",
        "count_list = dict(zip(unique, counts))\n",
        "print('Alghorythm marked', count_list[1], 'as attack and', count_list[1], \"packets as benign\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aa2qkaxI5xv"
      },
      "source": [
        "Чтение датасета для второй нейросети. KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC8ajhVkKaU5"
      },
      "source": [
        "data = []\n",
        "target = []\n",
        "last = []\n",
        "for name in os.listdir(path):\n",
        "    file = pd.read_csv(os.path.join(path, name))\n",
        "    for n, i in file.iterrows():\n",
        "        if not i[-1] in last:\n",
        "            last.append(i[-1])\n",
        "        if last.index(i[-1]) != 0:\n",
        "          a = []\n",
        "          for j in i[:-1]:\n",
        "              a.append(j)\n",
        "          data.append(a)\n",
        "          target.append(last.index(i[-1])-1)\n",
        "del file\n",
        "del last[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm1QbPYgI_U2"
      },
      "source": [
        "Обработка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8ugDynbN4Ah"
      },
      "source": [
        "data = np.array(data)\n",
        "target = np.array(target)\n",
        "data = data.reshape(data.shape[0], 78)\n",
        "print(data.shape, target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bk4IdtCA7ss"
      },
      "source": [
        "Загрузка нормализатора"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPSWb5R1HYDw"
      },
      "source": [
        "scaler = pickle.load(open('/content/drive/MyDrive/scaler.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWd6LwZbBAeT"
      },
      "source": [
        "Нормализация данных для второй нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCLkQqLq51hu"
      },
      "source": [
        "data = scaler.transform(np.nan_to_num(data).astype(float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-li25gkJCVe"
      },
      "source": [
        "Моделирование второй нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJNq4crHB8GM"
      },
      "source": [
        "knn = Sequential()\n",
        "knn.add(InputLayer((78,)))\n",
        "knn.add(BatchNormalization())\n",
        "knn.add(Dense(128, activation='relu'))\n",
        "knn.add(Dense(128, activation='relu'))\n",
        "knn.add(Dense(14, activation='softmax'))\n",
        "#optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "knn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Q88hM6JFMI"
      },
      "source": [
        "Тренировка второй нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2kScnK6_cTZ"
      },
      "source": [
        "knn.fit(data.astype(float), target, epochs=10, batch_size=256, verbose=1)\n",
        "\n",
        "knn.save('/content/drive/MyDrive/knn.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crodovKFJHpu"
      },
      "source": [
        "Получение данных со второй нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDOmyBHxF7xc"
      },
      "source": [
        "y_pred1 = knn.predict(data)\n",
        "y_pred = np.argmax(y_pred1, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV3IqZTrJohu"
      },
      "source": [
        "Вычисление метрик и количество классов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG3YS255F-cN"
      },
      "source": [
        "print('recall:', recall_score(target, y_pred, average='micro'))\n",
        "print('precision:', precision_score(target, y_pred, average='micro'))\n",
        "print('accuracy:', accuracy_score(target, y_pred))\n",
        "print('f1 score:', f1_score(target, y_pred, average='micro'))\n",
        "print('ROC AUC:', roc_auc_score(to_categorical(target), np.nan_to_num(y_pred1), multi_class='ovr'))\n",
        "print('матрица неточностей', confusion_matrix(target, y_pred))\n",
        "unique, counts = np.unique(y_pred, return_counts=True)\n",
        "count_list = dict(zip(unique, counts))\n",
        "print('Классы:')\n",
        "for i in count_list:\n",
        "  print(last[i], count_list[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bF6VJdRaaHG"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlRaA9XTaaD7"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSKcZe-KaaBH"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO9ES_jxT8T3"
      },
      "source": [
        "Здесь начинается проверка нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHOZ8IRfL_gv"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvzsgqr5VL20"
      },
      "source": [
        "Загрузка заранее обученной нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtWFL5iIMXN7"
      },
      "source": [
        "lstm = load_model('/content/drive/MyDrive/lstm.h5')\n",
        "knn = load_model('/content/drive/MyDrive/knn.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIzmB8aIVi6y"
      },
      "source": [
        "Загрузка данных для тестирования нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oNpuFZ_Mm-j"
      },
      "source": [
        "file = pd.read_csv('/content/drive/MyDrive/net/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
        "data = []\n",
        "target = []\n",
        "\n",
        "for n, i in file.iterrows():\n",
        "        a = []\n",
        "        for j in i[:-1]:\n",
        "          a.append(j)\n",
        "        data.append(a)\n",
        "        target.append(last.index(i[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWavdC4SX2sX"
      },
      "source": [
        "Обработка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdlqg0FNNoYI"
      },
      "source": [
        "data = np.array(data)\n",
        "target = np.array(target)\n",
        "data = data.reshape(data.shape[0], 78, 1)\n",
        "print(data.shape, target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PU5KkSHX519"
      },
      "source": [
        "Проверка нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLb1e3ayN6uf"
      },
      "source": [
        "model.evaluate(data, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdqWagy7X9st"
      },
      "source": [
        "Предсказание нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfNsLrYfOAYg"
      },
      "source": [
        "pred = model.predict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi5kun4zYBRM"
      },
      "source": [
        "Определение класса предсказанния нейросети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNnK1RJtRzgt"
      },
      "source": [
        "print(last[np.argmax(pred[5])])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}